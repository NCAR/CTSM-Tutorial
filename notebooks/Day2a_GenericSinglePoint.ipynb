{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2a: *Running a generic single-point case*\n",
    "\n",
    "This tutorial is an introduction to running single-point simulations of the Community Terrestrial Systems Model (CTSM) at locations that do not have preconfigured options.  It will guide you through setting up the required driver data (i.e. surface and atmosphere data) as well as setting up and submitting a single-point CTSM case.\n",
    "\n",
    "In the previous tutorial, `Day1a_GlobalCase`, we set up and ran a global CTSM case. Many of the steps required to run a single-point case are similar, with some changes and additional required steps which we will cover here.\n",
    "<br><br>\n",
    "\n",
    "### Questions about this tutorial? \n",
    "- Please post them on the [CTSM forum in the CESM Bulletin Board](https://bb.cgd.ucar.edu/cesm/forums/ctsm-clm-mosart-rtm.134/). Note that this resource will require you to register and log in so that you can be notified of responses to your inquiries. \n",
    "- You can also file issues on the [NCAR CTSM-Tutorial GitHub repository](https://github.com/NCAR/CTSM-Tutorial-2022).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this tutorial\n",
    "\n",
    "The tutorial has several components. Below you will find steps to: \n",
    "1. Generate subset surface and atmosphere data files at a single latitude and longitude point.\n",
    "2. Set up and submit a single-point case.\n",
    "\n",
    "Specifically, we will simulate the Harvard Forest site for one year using data extracted from global datasets that are available for CTSM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> This tutorial assumes you have completed the previous tutorials! Content in the Day 0a Git Started tutorial is required, and Day 0b NEON and Day 1 global tutorials are strongly recommended. If you haven't completed the Day 0 and Day 1 tutorials, go back and do these first.\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" markdown=\"1\">\n",
    "\n",
    "<b>TIP:</b>  Before we get started, make sure you're in a bash kernel \n",
    "<ul>\n",
    "    <li> Switch kernel (upper right of your current notebook)</li>\n",
    "    <li> Select either one of the Bash Kernels from the pop-up window</li>    \n",
    "    <li> Click select</li>    \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/NCAR/CTSM-Tutorial-2022/raw/main/images/kernel.png\" width=\"670\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Subset global surface and atmosphere files </h1>\n",
    "\n",
    "CTSM uses a surface data file to read in important grid cell-level information like vegetation, crop, and glacier grid cell fractions, the fractional cover of each plant functional type (PFT), and soil characteristics.\n",
    "\n",
    "A global surface data file is located and read by default for global CTSM cases, depending on the chosen _component set_ and _resolution_. To run CTSM at a single point, we will need to supply a surface data file at a specified latitude and longitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Remember from Day 1 that a <b>component set</b>, or colloquially a \"compset\", specifies a configuration for your case, including the component models, time period of simulation, and model physics options. The <b>resolution</b> defines the model resolution or grid size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similarly, when running a land-only simulation, that is when using a \"data atmosphere model\" (e.g., in _DATM mode_), with climate data (e.g. temperature, precipitation, solar radiation, etc.) driven by an input file, CTSM needs DATM files. We can also provide subset global DATM for single-point runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> It is not strictly required to provide CTSM/CIME with subset DATM data in order to run a single-point case, as CTSM can just use the global files. However, your simulations will run much faster if you use subset climate data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.1 Use <em>subset_data</em> to subset surface and DATM files</h2>\n",
    "\n",
    "We have created a python script, *subset_data*, which will subset default global surface and DATM files at a user-specified latitude and longitude.\n",
    "\n",
    "This script is located in the CTSM source code, in the `tools/site_and_regional` folder.\n",
    "\n",
    "Navigate here now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/CTSM/tools/site_and_regional\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the `subset_data` python script you must have some required python packages installed. On NCAR machines (like Cheyenne), you can load the NCAR python library, _ncar pylib_, by running `module load python` and then `ncar_pylib`. **This is not necessary in the cloud because your python environment is already configured in CESM-Lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> On NCAR or other machines, you can also use your own python environment if you want. Required third-party python packages are <i>scipy</i>, <i>xarray</i>, and <i>numpy</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the built-in print help to see what options are available for the subset data script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./subset_data --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are a lot of options, but for now we will just use a few of the most commonly used:\n",
    "\n",
    "**Type of subsetting:**<br>\n",
    "`point` : this tells the script to subset data at a single point (region is the other option)<br>\n",
    "\n",
    "**Location-related information:**<br>\n",
    "`--lat` : this tells the script which latitude to subset at (*must be between -90 and 90*)<br>\n",
    "`--lon` : this tells the script which longitude to subset at (*can be between 0 and 360 or -180 and 180*)<br>\n",
    "`--site` : optional, specifies a site name or tag<br>\n",
    "\n",
    "**Type of files to create:**<br>\n",
    "`--create-surface` : tells the script to subset surface data<br> \n",
    "`--create-datm` : tells the script to subset DATM data<br>\n",
    "\n",
    "**Time information:** <br>\n",
    "`--datm-syr` and `--datm-eyr`: starting and ending years for the DATM data to subset (*must be between 1901 and 2014*)<br>\n",
    "\n",
    "**Data management information:**<br>\n",
    "`--create-user-mods` : tells the script to create a *user_mods* directory (see below). Note that if you don't use this option, you will have to modify scripts in your simulation to point to the modifed files.<br> \n",
    "`--outdir` : specifies the directory to place subset data and user mods directory in<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the script with these options by running the line of code below \n",
    "A few notes:\n",
    "- We will use a latitude of 42.53562 and longitude of 287.82438 (i.e. Harvard Forest). Note that your latitude and longitude points do not have to be this precise for your own sites!\n",
    "\n",
    "- We will run the simulation from 2001 to 2002. Note that because we are only simulating years around present day, we are not using land use change. If you want to run a transient simulation, you will also need to create land use data (the create_landuse option).\n",
    "\n",
    "- This is also pretty time consuming, so we'll use `qcmd_serial` here to put this in the queue on a single processor.  \n",
    "\n",
    "- As before, please be patient while this runs, and don't worry if you see `WARNING: No dominant pft type is chosen.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcmd_serial -- ./subset_data point --lat 42.53562 --lon 287.82438 --site my_point --create-surface --create-datm --datm-syr 2001 --datm-eyr 2002 --create-user-mods --outdir /scratch/$USER/my_subset_data\n",
    "\n",
    "echo \"------------------------\"\n",
    "echo \"Successfully subset data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the speed of your computing system, it may take a bit of time to subset all the climate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.2 Check on the subset files </h2>\n",
    "\n",
    "Once the subsetting has successfully finished, let's navigate to the specified output directory to check on the data that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/$USER/my_subset_data\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a surface data file (e.g. *surfdata_0.9x1.25 ... .nc*) and two folders: **datmdata** and **user_mods**. \n",
    "\n",
    "* **datmdata** houses the subset DATM files\n",
    "* **user_mods** is a directory created that houses several files we will use to set up our single-point case\n",
    "\n",
    "Let's navigate into the **user_mods** directory to look at the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd user_mods\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see three files: *shell_commands*, *user_nl_clm*, and *user_nl_datm_streams*.  \n",
    "\n",
    "The *shell_commands* file contains *xmlchange* commands required to set up a single point case at the specified latitude and longitude.\n",
    "\n",
    "Take a look at this file if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat shell_commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many of the xml commands are changing aspects of the model configuration that are communicated to **[CIME](https://github.com/ESMCI/cime)** (Common Infrastructure for Modeling the Earth), which is the infrastructure that generates model executables and associated input files. Below are explanations of the commands included in this script. \n",
    "\n",
    "`./xmlchange CLM_USRDAT_DIR` - this tells CIME the location of an argument *CLM_USRDAT_DIR* which we can use to specify the main directory of subset data files  \n",
    "\n",
    "`./xmlchange PTS_LON` and `./xmlchange PTS_LAT` - this tells CIME that we are running at a specified latitude and longitude  \n",
    "\n",
    "`./xmlchange MPILIB` - this specifies a specific MPI (*Message Passing Interface*) library to use required for single-point runs on NCAR machines.  \n",
    " \n",
    "   \n",
    "If you remember from the Day 1 tutorial, *user_nl_clm* is a Fortran namelist file used to set up different namelist options for CLM. Here, we are using it to specify the location of our subset surface data. Note the use of the variable `$CLM_USRDAT_DIR` set up in the *shell_commands* file.\n",
    "  \n",
    "    \n",
    "Similarly, *user_nl_datm_streams* specifies the location and a few other options for our subset DATM data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this **user_mods** directory when we create our single-point case (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> If for whatever reason you end up moving the subset data directory (i.e. here <b>/scratch/$USER/my_subset_data</b>), you will need to modify the xmlchange command that specifies the <i>CLM_USRDAT_DIR</i> to be the full path to the directory's new location. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Create a single-point CTSM case </h1>\n",
    "\n",
    "Now that we have our subset data ready to go, we can set up our single-point case with CIME.\n",
    "\n",
    "The steps required here are very similar to the global case that we set up in the Day 1 tutorial, with a few differences. Mainly, we are going to specify a `--user-mods-dir` in our `./create_newcase` command as the full path to the **user_mods** folder we just created with *subset_data* to point to all the data required to run the single point (rather than global) simulation. We will also choose a different component set and resolution.\n",
    "\n",
    "<h2> 2.1. Create the case </h2>\n",
    "\n",
    "As in our Day 1 Tutorial, we will navigate into the CTSM **scripts** directory to run the `create_newcase` script. Today we are going to be running a CLM-BGC simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/CTSM/cime/scripts\n",
    "./create_newcase --case ~/clm_tutorial_cases/I2000_CTSM_singlept --res CLM_USRDAT --compset I2000Clm51BgcCrop --run-unsupported --user-mods-dirs /scratch/$USER/my_subset_data/user_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command should look fairly familiar to you, with some updated values and arguments.\n",
    "\n",
    "`--res` - defines the model resolution, or grid:\n",
    "* we are now using `CLM_USRDAT`, which should be used when we have user-specified domain (i.e. a subset surface data)\n",
    "\n",
    "`--compset` - defines the component set for the case:\n",
    "* `I2000Clm51BgcCrop` is an alias that describes using year 2000 initialization time, data-driven atmosphere (GSWP3v1 data), CLM 5.1 BGC with prognostic crop, along with some other component settings.\n",
    "\n",
    "`--user-mods-dirs` - this is where we tell CIME where our **user mods** directory is.\n",
    "* it should be the path to the directory that was created during our *subset_data* scripting\n",
    "* the namelist files (i.e. *user_nl_clm* and *user_nl_datm_streams*) will be copied into the case directory, and the commands within *shell_commands* will be executed. Remember that the information included in these files ensures the model uses the data subset for our site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Depending on your machine (e.g. this is required on Cheyenne), you may also have to provide a project id (<code>--project {PROJECT_ID}</code>) which specifies accounting or directory permissions when on a batch system. By default the script uses the shell environment variable <code>$PROJECT</code>, which can be set in your bash profile\n",
    "\n",
    "<i>Remember that you can see script parameters and definitions using</i> <code>./create_newcase --help</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.2. Set up the case and build the executable </h2>\n",
    "\n",
    "As with our global case, we will change into our case directory, set up our case (`./case.setup`) and then build the executable with qcmd (`qcmd -- ./case.build`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/clm_tutorial_cases/I2000_CTSM_singlept\n",
    "./case.setup\n",
    "./xmlchange MPILIB=impi\n",
    "qcmd -- ./case.build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> You'll notice we changed the <code>MPILIB</code> parameter from what was set in the <b>user_mods</b>. This is a special change we have to make to run on this cloud system. On NCAR machines (e.g. Cheyenne) you should keep <code>MPILIB=mpi-serial</code>.\n",
    "</div>\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "You can read on, but before executing any code blocks in the notebook **wait for the model to build.**\n",
    "This can take a while, especially while you're wating for your `qcmd` job to start and as code for the land model compiles.\n",
    "\n",
    "You'll see text stating `MODEL BUILD HAS FINISHED SUCCESSFULLY` when it's finished.\n",
    "\n",
    "<h2> 2.3. Customize the case </h2>\n",
    "\n",
    "As in our global case, we will invoke a few XML commands to update some runtime values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./xmlchange STOP_OPTION=nyears\n",
    "./xmlchange STOP_N=1\n",
    "./xmlchange RUN_STARTDATE='2001-01-01'\n",
    "./xmlchange DATM_YR_ALIGN=2001\n",
    "./xmlchange DATM_YR_START=2001\n",
    "./xmlchange DATM_YR_END=2002\n",
    "./xmlchange PIO_REARRANGER_LND=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Spinup </h3>\n",
    "\n",
    "When running a model like CLM, the initial conditions (i.e. state variables like carbon and nitrogen pools and soil moisture) have an impact on the results of the simulation. Often, we don't know the precise values of these initial conditions. To get around this issue, we can initialize the model with arbitrary values and then run the model with some cycle of atmospheric forcing for many years (e.g. 200) until the model attains an equilibrium state. Then, we can simulate the model response to some perturbation (e.g. changing climate, CO<sub>2</sub>, etc.). This process -- establishing an equilibrium state -- is called _spinup_.\n",
    "\n",
    "We have already run such a spinup simulation at Harvard Forest. We can see how the soil C pools evolved for this simulation over time.\n",
    "\n",
    "*Note that you'll see **AD mode** for accelerated decomposition mode and **Post-AD mode** on this figure. See the tip below or visit the [Model Equilibrium and its Acceleration](https://escomp.github.io/ctsm-docs/versions/release-clm5.0/html/tech_note/Decomposition/CLM50_Tech_Note_Decomposition.html#model-equilibration-and-its-acceleration) section of the CLM Tech Note for more information*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/NCAR/CTSM-Tutorial-2022/raw/main/images/ad_mode.png\" width=\"525\" height=\"375\" alt=\"Evolution of different C pools during the accelerated decomposition spinup.\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The steady-state, or equilibrium, size of carbon (C) and nitrogen (N) pools are proportional to their tunrover time. This spinup simulation was conducted using \"accelerated decomposition\", or \"AD\" mode. This accelerates the turnover time of \"slow\" ecosystem C and N pools (soil, wood, and coarse woody debris) so they come into equilibrium more quickly. \n",
    "\n",
    "We ran the model in AD mode for 100 years cycling through atmospheric forcing for 1981 to 2000.\n",
    "AD mode was invoked with the commands: <code>./xmlchange CLM_FORCE_COLDSTART=on</code> and <code>./xmlchange CLM_ACCELERATED_SPNIUP=on</code>.\n",
    "\n",
    "We then ran a \"post-AD\" simulation (using the end of our \"AD\" simulation as the starting point) for another 100 years with <code>./xmlchange CLM_ACCELERATED_SPINUP=off</code>. In returning the turnover times of slow C and N pools to their intentended rates, we have to adjust the pool sizes from their \"AD\" steady-state. For example if the turnover of \"passive\" soil C was 10x faster in AD mode, the passive soil C pool needs to be 10x larger starting the post-AD simulation (the model automatically handles this conversion for you). Then we ran the simulation for another 100 years to allow the state variables to equilibrate with non-accelerated decomposition.  In post-AD mode the history files are monthly, whereas AD output is set to annual averages by default.  This difference in history file output frequency is reflected in the variability in the post-AD GPP output.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After spin up is complete, we have to tell CIME to use the spinup simulation's end point as the starting point, or initial conditions, for our simulation. \n",
    "\n",
    "We do this via the <i>user_nl_clm</i> file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"finidat='/scratch/data/day2/finidat_file/I2000_CTSM51_spinup.clm2.r.0281-01-01-00000.nc'\" >> user_nl_clm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> You can also do this from the command line in a terminal window using any text editing software (e.g. vi, emacs, etc.)\n",
    "</div>\n",
    "\n",
    "Let's check the file to make sure our command worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat user_nl_clm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.4. Submit the case </h2>\n",
    "\n",
    "Finally, let's submit the case as we did in Day 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./case.submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a confirmation that it successfully submitted.\n",
    "\n",
    "### Congratulations! You've created and submtted a single-point CLM case!\n",
    "\n",
    "You can check the status of your case as in previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qstat -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Once your jobs are complete (or show the 'C' state under the 'Use' column, which means complete), we can check the CaseStatus file to ensure there were no errors and it submitted, ran, and completed successfully.  To do this, we'll 'tail' the end of the CaseStatus file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail ~/clm_tutorial_cases/I2000_CTSM_singlept/CaseStatus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from Day1a that you can check on files in your runs in the **scratch/{USER}/{CASE_NAME}/run** directory (e.g. `/scratch/$USER/I2000_CTSM_singlept/run`). \n",
    "\n",
    "Archived history files will be in the **scratch/{USER}/archive/{CASE_NAME}/lnd/hist** directory (e.g. `/scratch/$USER/archive/I2000_CTSM_singlept/lnd/hist`).\n",
    "\n",
    "Check on the status of your case to see that it's running as expected\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, check out the next Generic Single Point Tutorial [2b_GenericSinglePoint_Visualization](Day2b_GenericSinglePoint_Visualization.ipynb) to walk through how to visualize and analyze some of the output produced. **Note that you don't need to wait for this job to finish before moving on to the Day2b tutorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /scratch/$USER/I2000_CTSM_singlept/run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /scratch/$USER/I2000_CTSM_singlept/run/lnd.log.288.220523-144323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
